---
layout: post
title: "Machine Learning: Decision Tree Classifier with SciKit-Learn"
---
<img src="/Images/DTC/DTC_Head.jpg" class="inline"/><br>
The Great Decider. 

## Decision Tree Algorithm - CART
Classification and regression trees (CART) Algorithm
CART = supervised learning models used for problems involving classification and regression

## Classification Tree
Sequence of if-else questions about features
Objective is to infer class labels
Capture non-linear relationships between features and labels
No feature scaling required

## Classification Model - Approach
Dataset, break into Training and Test sets. Use Training dataset with a learning algorithm (in our case, the scikit-learn's DecisionTreeClassifier) to create a model via induction, which is then applied to make predictions on the test set of data through deduction. 

## Model Concerns 
Selecting the right problem
Providing adequate data
Properly preparing data
Preventing algorithm bias

## Performance Metrics

`Accuracy`  = `# correct predictions / # total predictions`

`Error rate` = `# number wrong predictions / # total predictions`

Goal - Highest accuracy with lowest error rate

## Tree Data Structure - Fundamentals

<img src="/Images/DTC/tree_anatomy.jpeg" class="inline"/><br>

Unlike common linear data structures, like `lists` and `arrays`, a `Tree` is a non-linear, hierarchical method of storing/modeling data. Visually, you can picture an evolutionary tree, a `document object model (DOM)` from HTML, or even a flow chart of a company hierarchy. In contrast to a biological tree originating of kingdom plantae, the data structure tree has a simple anatomy. 

A tree consists of **nodes** and **edges**. 

There are 'specialized' kinds of nodes we classify by unique names which represent their place on the hierarchy of our structure. The **root** node, is a single point of origin for the rest of the tree. Branches can extend from nodes and link to other nodes, with the **link** referred to as an edge. The node which accepts a link, is said to be a **child** node, and the originating node is the **parent**. A single node may have one, two, or no children. If a node has no children, but does have a parent, it is called a **leaf**. Some also refer to **Internal nodes**, which have one parent and two children. 

Beyond the core anatomy, the tree has unique features; **depth** and **height**. **Depth** refers to the spatial attributes of an individual node in relation to the root, meaning, how many links/edges are between the specific node and the root node. The **height**, refers to the number of edges in the longest possible path of the tree, similar to finding the longest carbon chain back in organic chemistry to determine the IUPAC name of the compound.

## Decision Tree
<img src="/Images/DTC/Decision_tree_anatomy.jpg" class="inline"/><br>
A decision tree, allows us to run a series of tests on a data point with many attributes to be tested. Each node of this tree, would represent some condition of an attribute to test, and the edges/links are the results of this test constrained to some kind of binary decision. For example, if we had a dataset with rich features about a human, we could ask many questions about that person.  

## Decision Regions
`Decision Boundary`: Point of transition from one decision region to another, aka one class/label to another
`Decision Region`: Space where instances become assigned to particular class
`Negative Decision Region`
`Positive Decision Region`

## Feature Selection

Identification of which features/columns with the highest weights in predictive power. 

Removing low variance features
`from sklearn.feature_selection import VarianceThreshold`

In our case, the CART algorithm will do the feature selection for us through the `Gini Index` or `Entropy` which measure how pure your data is partitioned through it's journey to a final leaf node classification.  

## SciKit Learn DecisionTreeClassifier
```Python3
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
```

#Steps
1 - Imports

2 - Test and Train data

3 - Instantiate a Decision Tree Classifier

4 - Fit data

5 - Predict

6 - Check Accuracy

## Load Dataset
```Python3
# X contains predictors, y holds the classifications
X, y = dataset.data, dataset.target
features = iris.feature_names
```
## Import DecisionTreeClassifier 
```Python3
from sklearn.tree import DecisionTreeClassifier
```

## Split dataset into 80% train, 20% test sets
```Python3
X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)
```
## Instantiate a DecisionTreeClassifier
'dt' with a maximum depth of 6, random state for reproducibility
```Python3
dt = DecisionTreeClassifier(max_depth=6, random_state=1)
```
## Fit dt to the training set
```Python3
dt.fit(X_train, y_train)
```

## Predict test set labels
```Python3
y_pred = dt.predict(X_test)
```

## Print predictions
```Python3
print(y_pred[0:5])
```

## Check accuracy
```Python3
acc = accuracy_score(y_test, y_pred)
print("Test set accuracy: {:.2f}".format(acc))
```
